---
title: "Sta 141A Project"
author: "Zhekai Fan"
date: "2025-02-02"
output:
  html_document:
    toc: true
    toc_float: true
  pdf_document: default
---   
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(dplyr)
library(tidyr)
library(readr)
library(tidyverse)
library(caret) 
library(xgboost)
library(ROCR)
library(ROSE)
library(randomForest)
library(pROC)
```
# Section 1 Introduction

In this project, we have been provided a data set with 18 sessions. In each of the 18 sessions, 8 variables are available for each trial and they are: feedback type,contrast left, contrast right,time,spks,mouse name, data of experience and brain area. 

In this project, we are asked to create a model which can predict the feedback type using the neural activity data (i.e., spike trains in `spks`), along with the stimuli (the left and right contrasts). 

They are mainly three parts of my project, namely **Exploratory data analysis **, **data integration and Model training ** and  **prediction**.
```{r}
#read the data
session = list()
for(i in 1:18) {
  session[[i]] = readRDS(paste('C:/Users/sz_hu/Desktop/computer/2024winter/sta141/project/datas/session', i, '.rds', sep=''))
}
#read the test data given by the instructor
sta141A_test_data=list()
for(i in 1:2) {
  sta141A_test_data[[i]] = readRDS(paste('C:/Users/sz_hu/Desktop/computer/2024winter/sta141/project/testdata/test', i, '.rds', sep=''))
}

```


# Section 2 Exploratory data analysis
In this section, we will go through all 18 sessions and try to find is there any correlation between the feedback types(1 and -1) and the neural activity data and stimuli(left and right contrasts). Let's go through it step by step.
```{r}
#count how many trails for each session.
trials_df <- data.frame(
  Session = 1:18, 
  Number_of_Trials = sapply(1:18, function(i) length(session[[i]]$time))
)
name_across_session <- data.frame(
  name = sapply(1:18, function(i) (session[[i]]$mouse_name))
)
```

**Summary of the whole data set **
```{r}
# number of neurons
number_of_neurons <- sapply(1:18, function(i) nrow(session[[i]]$spks[[1]]))
num_feedbacktype_1 <- sapply(1:18, function(i) sum(session[[i]]$feedback_type == 1))
num_feedbacktype_neg1<- sapply(1:18, function(i) sum(session[[i]]$feedback_type == -1))
Sessiondata=tibble(trials_df,number_of_neurons,name_across_session,num_feedbacktype_1,num_feedbacktype_neg1)
print(Sessiondata[1:18,])
```

```{r}
# neural activities
neuralactivities <- lapply(1:18, function(i) session[[i]]$spks)
# we calculate the success rate
percentofsuccess_df <- data.frame(
  name = sapply(1:18, function(i) session[[i]]$mouse_name),
  percentofsuccess = sapply(1:18, function(i) mean(session[[i]]$feedback_type == 1))
)

ggplot(percentofsuccess_df, aes(x = factor(name, levels = unique(name)), y = percentofsuccess*100, group = name)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "red", size = 2) +
  labs(title = "Percent of Success Across Sessions", x = "Mouse Name", y = "Percent of Success")

# calculate the average success rate for each mice.
average_df <- percentofsuccess_df %>%
  group_by(name) %>%
  summarise(avg_percent = mean(percentofsuccess))
print(paste(
  "The average success rate for", average_df$name[1], "is:", round(average_df$avg_percent[1],3), ".", 
  "The average success rate for", average_df$name[2], "is:", round(average_df$avg_percent[2],3), ".",
  "The average success rate for", average_df$name[3], "is:", round(average_df$avg_percent[3],3), ".",
  "The average success rate for", average_df$name[4], "is:", round(average_df$avg_percent[4],3), "."
))

```
Lederberg has the highest success rate. **Why Lederberg is so smart compare with other mouse?** 
So the hint in making prediction model: It may because each time it have a higher spike rate,  or some specific brain areas are highly active for Lederberg.

Table and graph which shows the percent of active spike across one whole session for each mouse.
```{r}
# calculate the proportion of active neuron areas.
percentofneurons_df <- data.frame(
  name = sapply(1:18, function(i) session[[i]]$mouse_name),
  percentofneurons = sapply(1:18, function(i) {
    total_spikes <- sum(unlist(session[[i]]$spks) == 1)
    total_neurons <- number_of_neurons[i] * 40
    total_spikes / (total_neurons*length(session[[i]]$time))
  }
))
# calculate the average active neuron area
averageneuron_df <- percentofneurons_df %>%
  group_by(name) %>%
  summarise(avg_neuronpercent = mean(percentofneurons))
print(averageneuron_df)

ggplot(averageneuron_df, aes(x = factor(name, levels = unique(name)), y = avg_neuronpercent)) + 
  geom_point(color = "blue", size = 4) + 
  labs(title = "Percent of Neural Activities for Different Mice",
       x = "Mouse Name",
       y = "Percent of Neural Activities") +
  theme_minimal()  

# calculate how many unique brain areas are there in a session.
numberofuniquenames <- sapply(1:18, function(i) length(unique(session[[i]]$brain_area)))
```

However Lederberg didn't have a high percent of active neuron. We may need to find other directions to investigate. 

Here i define the average spike in each section: **average spike=total number of spikes(1) in a trail/number of trails**
```{r}
# Here i went through the 18 sessions and want to export the data to integrate a new dataframe, where it include the feedback type, decision, and average spikes. Decision is determined by the below techniques.
all_dat <- bind_rows(lapply(1:18, function(s) {
  data <- session[[s]]
  n_obs <- length(data$feedback_type)
  tibble(
    feedback_type = as.factor(data$feedback_type),
    avg_spikes = sapply(1:n_obs, function(i) mean(rowSums(data$spks[[i]]), na.rm = TRUE))
  )
}))
n=5081
feedback_1 <- all_dat %>% filter(feedback_type == 1)  # Feedback type = 1
feedback_neg1 <- all_dat %>% filter(feedback_type == -1)  # Feedback type = -1

#using the same way, we just create a dataframe which involves name of mouse
namedata <- bind_rows(lapply(1:18, function(s) {
  data <- session[[s]]
  n_obs <- length(data$feedback_type)
  tibble(
    feedback_type = as.factor(data$feedback_type),
    name=data$mouse_name,
    avg_spikes = sapply(1:n_obs, function(i) mean(rowSums(data$spks[[i]]), na.rm = TRUE))
  )
}))

# Compute the average of avg_spikes for each feedback type
avg_spikes_feedback_1 <- mean(feedback_1$avg_spikes, na.rm = TRUE)
avg_spikes_feedback_neg1 <- mean(feedback_neg1$avg_spikes, na.rm = TRUE)

avg_spikes_Cori=mean((namedata %>% filter(name == "Cori"))$avg_spikes, na.rm = TRUE)
avg_spikes_Forssmann=mean((namedata %>% filter(name == "Forssmann"))$avg_spikes, na.rm = TRUE)
avg_spikes_Hench=mean((namedata %>% filter(name == "Hench"))$avg_spikes, na.rm = TRUE)
avg_spikes_Lederberg=mean((namedata %>% filter(name == "Lederberg"))$avg_spikes, na.rm = TRUE)
# Print results
print(paste("Average spikes for feedback_type = 1:", avg_spikes_feedback_1))
print(paste("Average spikes for feedback_type = -1:", avg_spikes_feedback_neg1))
print(paste("we see that the average spikes for feedback_type 1 is bigger than the average spikes for feedtype -1."))
print(paste("Average spikes for Cori:", avg_spikes_Cori))
print(paste("Average spikes for Forssmann:", avg_spikes_Forssmann))
print(paste("Average spikes for Hench:", avg_spikes_Hench))
print(paste("Average spikes for Lederberg:", avg_spikes_Lederberg))
```
This is interesting: since we **we don't see a clear linear relationship between average spikes and success rate for mouse**


Do Some neural areas are really important in success and failure?

Do different contrast impact the rate of success?

Here we can see a table which summarize the distribution of contrast difference.
```{r}
#using the same techinic, we just simply created a dataframe which involves the feedback type and the absolute value of contrast difference.
contrastdataframe <- bind_rows(lapply(1:18, function(s) {
  data <- session[[s]]
  n_obs <- length(data$feedback_type)
  tibble(
    feedback_type = as.factor(data$feedback_type),
    contrast_diff=as.numeric(abs(data$contrast_left-data$contrast_right)))
}))
#And we count what is number for each contrast difference and calculate the percentage
contrast_summary <- contrastdataframe %>%
  count(contrast_diff) %>%  # Count occurrences of each contrast_diff
  mutate(
    perc = n / sum(n),  # Compute percentage
    labels = paste0(round(perc * 100, 2), "%")  # Format as percentage string
  )

contrast_summary <- contrast_summary %>%
  arrange(desc(contrast_diff))  #And we arrage it from 1 to 0

# Print nicely
print(contrast_summary)
```
How does the contrast difference affect the success rate?
```{r}
# Compute success rate along with count and percentage
contrast_summary <- contrastdataframe %>%
  group_by(contrast_diff) %>%#group by contrast difference, so we calculate one by one
  summarise(
    n = n(),  # Count occurrences of each contrast_diff
    num_success = sum(feedback_type == 1),  # how many success feedback are there in total.
    success_rate = num_success / n,  # Calculate success rate
    )%>%arrange(desc(contrast_diff))  # And we arrage it from 1 to 0

# Display the result
print(contrast_summary)


```
Sp  we see that the success rate is really high when there are high difference between the 2 contrasts, might because give mice a higher stimulation.

What is the difference among each trail? For example, average spike rate?

```{r}
avg_spikes_per_session=list()
# Loop through each session
for (s in 1:length(session)) {

  data <- session[[s]]
  numberoftrial <- length(data$feedback_type)
  #calculate what is the average spikes rate for each trail
  avg_spikes_per_trail = sapply(1:numberoftrial, function(i) mean(rowSums(data$spks[[i]]), na.rm = TRUE))
  #calculate what is the average spikes rate for each session by taking the average of average per trail
  avg_spikes_per_session[[s]]=mean(avg_spikes_per_trail)
}

avg_spikes_session <- data.frame(
  Session = 1:18,#make avg_spikes_per_session not a list
  AvgSpikes = unlist(avg_spikes_per_session)
)
ggplot(avg_spikes_session, aes(x = Session, y = AvgSpikes)) +
  geom_line(color = "blue") +
  geom_point(color = "red") +
  labs(title = "Average Spikes per Session",
       x = "Session Number",
       y = "Average Spikes") +
  theme_minimal()

```
What are the brain areas with neurons recorded in each session?

Firstly, we want to find how many unique brain region are there in 18 sessions.

```{r}
# get all brain information from 18 sessions
all_brain_areas <- unlist(lapply(session, function(data) data$brain_area))

# find the unique brain areas
unique_brain_areas <- unique(all_brain_areas)

# calculate how many unique brain areas are there
num_unique_brain_areas <- length(unique_brain_areas)

# print the result
cat("number of unique brain areas throughout the 18 sessions:", num_unique_brain_areas, "\n")

#we want to see the name of these brain areas
print(unique_brain_areas)
```
then we go through 18 sessions,see what neuron areas do each session only have.
```{r}
# Combine session data into one dataframe
session_data <- do.call(rbind, lapply(seq_along(session), function(i) {
  data.frame(session_id = i, brain_area = session[[i]]$brain_area)
}))

#factorize brain area
session_data$brain_area <-factor(session_data$brain_area, levels = unique(session_data$brain_area))
ggplot(session_data, aes(x = session_id, y = brain_area)) +
  geom_point(size = 2) +  
  theme_minimal() +
  labs(
    title = "What are the brain areas in each session?",
    x = "Session number",
    y = "Brain Area"
  ) +
  scale_x_continuous(breaks = 1:18) +  # Show all session numbers
  theme(
    axis.text.x = element_text(size = 11, angle = 0, hjust = 0.5),
    axis.text.y = element_text(size = 5),  # Make brain area text smaller to avoid overlap
    panel.grid.major = element_line(color = "grey", linetype = "dashed"),  # add a boundary line
    panel.grid.minor = element_blank()  # Remove grid lines
  )
```
And we try to find it there certain brain area that is significant for the success and failure for all mice?
```{r}
# get all mouse name
mouse_names <- unique(sapply(session, \(x) x$mouse_name))

# calculate what are the brain areas that have the most difference between success and failure.
all_mice_brain_areas <- lapply(mouse_names, \(mouse) {
  mouse_data <- bind_rows(lapply(which(sapply(session, \(x) x$mouse_name == mouse)), \(s) {
    data <- session[[s]]#since if both contrast are zero, just randomize select which one can be considered as success, we need to remove the type of trails since it doesn't related to how well did the mice do in the trail. 
    valid_trials <- which(!(data$contrast_left == data$contrast_right & data$contrast_left != 0))
    #extract the valid trails and calculate the sum of spikes when the feedback type is 1 and negative
    success_spikes <- rowSums(Reduce(`+`, lapply(valid_trials[data$feedback_type[valid_trials] == 1], \(i) data$spks[[i]])))
    failure_spikes <- rowSums(Reduce(`+`, lapply(valid_trials[data$feedback_type[valid_trials] == -1], \(i) data$spks[[i]])))
    #success ratio is calculate by success spikes divided by the sum of success and failure spikes
    tibble(Brain_Area = data$brain_area, Success_Ratio = success_spikes / (success_spikes + failure_spikes + 1e-6))
  })) %>% filter(!is.na(Success_Ratio))   #filter out when success ratio is na
  
  #we calculate the success ratio for each brain area and choose the top five
  mouse_data %>%
    group_by(Brain_Area) %>%
    summarise(Avg_Success_Ratio = mean(Success_Ratio, na.rm = TRUE), .groups = "drop") %>%
    arrange(desc(Avg_Success_Ratio)) %>%
    slice_head(n = 5) %>%  # take the top five brain areas
    mutate(Mouse_Name = mouse)
})

# integrate all mice brain areas
final_brain_area_data <- bind_rows(all_mice_brain_areas)

#  draw the top five brain areas for each mice using bar graph, y axis are their success ratios.
ggplot(final_brain_area_data, aes(x = reorder(Brain_Area, -Avg_Success_Ratio), y = Avg_Success_Ratio, fill = Mouse_Name)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "the top active brain areas for each mice",
       x = "brain area", y = "success spike ratio (Success Ratio)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~ Mouse_Name, scales = "free_x")  
```


This result can't predict for the whole model, even not useful for one mouse. Firstly, i claim that each mice's brain is functioning very differently. So we can't take the top 5 area of mice 1 to predict mice 2. Also in the 18 sessions, each of session record different brain area, so sometimes even one brain area may significant, it is not recorded in this session. So we need to find some other predictors to build the prediction model.
**One insight:We can seperate the session into the session that do contain these top areas and session that didn't contain these top areas. And build model for each of them.** However, this may become quite complicate.


Is the mice become tired after trails?
```{r}
#Create a dataframe which include the average spike rate for each time spot.
average_pertrail_persession <- bind_rows(lapply(1:18, function(s) {
  data <- session[[s]]
  n_obs <- length(data$feedback_type)
  tibble(
    session=s,
    num_trail=1:n_obs,
    avg_spikes = sapply(1:n_obs, function(i) mean(rowSums(data$spks[[i]])))
  )
}))
average_pertrail_persession$session <- as.factor(average_pertrail_persession$session)

# draw the graph: trails vs average spike rate, draw for each session
ggplot(average_pertrail_persession, aes(x = num_trail, y = avg_spikes)) +
  geom_line(size = 1, color = "blue") +
  facet_wrap(~ session) + geom_smooth(method = "loess", color = "red", se = FALSE)+
  labs(title = "Average Spikes per Trial Across Sessions",
       x = "Trial Number", y = "Average Spikes") +
  theme_minimal()



```



we see the average spike rate decrease over trial, **mice may become tried**.




Since investigating on certain brain area may become quite complicate. We want to see a more obvious pattern that differentiate the success and failure. One way based on my **domain knowledge, is that brain may become more active if we want to process information, that is, make any actions.**
So,**Is there any difference in average spike rates over time between successful (feedback type = 1) and failure (feedback type = -1) trials?**
```{r}
# create a list to record the average spike rate for all sessions
average_spike_list <- lapply(1:18, function(s) {
  data <- session[[s]]  # get each session data 
  
  # find the indices for feedback one and negative one.
  feedback_1_indices <- which(data$feedback_type == 1)
  feedback_neg1_indices <- which(data$feedback_type == -1)
  
  # export the feedback one and feedback negative one trails using the indices
  feedback_1_trials <- data$spks[feedback_1_indices]  
  feedback_neg1_trials <- data$spks[feedback_neg1_indices]

  # calculate the average spike rate for each time spot
  avg_spike_1 <- colMeans(Reduce("+", feedback_1_trials) / length(feedback_1_trials))
  avg_spike_neg1 <- colMeans(Reduce("+", feedback_neg1_trials) / length(feedback_neg1_trials))
  # build a dataframe
  tibble(
    time = rep(1:40, 2),  # there are 40 time spots in total
    avg_spike = c(avg_spike_1, avg_spike_neg1),
    feedback_type = rep(c("Feedback 1", "Feedback -1"), each = 40)
  )
})
 

final_spike_data <- bind_rows(average_spike_list, .id = "session_number")

# calculate the average spike rate over sessions
final_spike_summary <- final_spike_data %>%
  group_by(time, feedback_type) %>%
  summarise(
    final_avg_spike = mean(avg_spike, na.rm = TRUE),
    .groups = "drop"
  )

# print  the final result
print(final_spike_summary)
#draw the average spike rate by 40 time spots.
ggplot(final_spike_summary, aes(x = time, y = final_avg_spike, color = feedback_type)) +
  geom_line(size = 1) +
  labs(title = "Comparison of Average Spike Rate (Feedback 1 vs Feedback -1) by time",
       x = "Time spots", y = "Average Spike Rate across sessions") +
  theme_minimal() +
  scale_color_manual(values = c("Feedback 1" = "blue", "Feedback -1" = "red")) +
  theme(legend.title = element_blank())  
```

**This finding is insightful!**

See how are the comparisons for single mice.
```{r}
# create a list to record the average spike rate for all sessions, here we use the same code as above
average_spike_list <- lapply(1:18, function(s) {
  data <- session[[s]] 
  feedback_1_indices <- which(data$feedback_type == 1)
  feedback_neg1_indices <- which(data$feedback_type == -1)
  feedback_1_trials <- data$spks[feedback_1_indices] 
  feedback_neg1_trials <- data$spks[feedback_neg1_indices]

  avg_spike_1 <- colMeans(Reduce("+", feedback_1_trials) / length(feedback_1_trials))
  avg_spike_neg1 <- colMeans(Reduce("+", feedback_neg1_trials) / length(feedback_neg1_trials))
  #the change we make is that we add a name variable to our dataframe
  tibble(
    time = rep(1:40, 2),  # 40 time spot
    avg_spike = c(avg_spike_1, avg_spike_neg1),
    feedback_type = rep(c("Feedback 1", "Feedback -1"), each = 40),name=rep(session[[s]]$mouse_name,80)
  )
})
 

final_spike_data <- bind_rows(average_spike_list, .id = "session_id")
mouse_avg_spike_data <- final_spike_data %>%
  group_by(name, time, feedback_type) %>%
  summarise(avg_spike = mean(avg_spike, na.rm = TRUE), .groups = "drop")
#also group by name
final_spike_summary_mouse <- final_spike_data %>%
  group_by(time, feedback_type,name) %>%
  summarise(
    final_avg_spike = mean(avg_spike, na.rm = TRUE),  
    .groups = "drop"
  )

mouse_spike_diff <- mouse_avg_spike_data %>%
  spread(feedback_type, avg_spike) %>%
  mutate(spike_diff = `Feedback 1` - `Feedback -1`)  

spike_diff_summary <- mouse_spike_diff %>%
  group_by(time) %>%
  summarise(mean_spike_diff = mean(spike_diff, na.rm = TRUE), .groups = "drop")

ggplot(mouse_avg_spike_data, aes(x = time, y = avg_spike, color = feedback_type)) +
  geom_line(size = 1) +
  facet_wrap(~ name, scales = "free_y") +  #each mouse has one graph
  labs(title = "Average Spike Rate by Mouse (Feedback 1 vs Feedback -1) by time",
       x = "time", y = "Average Spike Rate") +
  theme_minimal() +
  scale_color_manual(values = c("Feedback 1" = "blue", "Feedback -1" = "red")) +scale_x_continuous(breaks = seq(0, max(spike_diff_summary$time), by = 4))+#want to see how the average spike rate changed detailly, so we divided x axis by 4
theme(legend.title = element_blank())  


```


After 10 seconds, this finding is so impressive! Every mice  have a lot difference between their average spike rate for feedback negative 1 and feedback 1.
Now we want to create the diff graph for each mice, where diff=average when feedback1 minus average when feedback -1.

```{r}

# we create a graph which shows the difference in average spike rate between feedback types among different mouse.
ggplot(mouse_spike_diff, aes(x = time, y = spike_diff, color = name)) +
  geom_line(size = 1) +
  labs(title = "Difference in Average Spike Rate (Feedback 1 - Feedback -1) across time",
       x = "Time", y = "Spike Rate Difference") +scale_x_continuous(breaks = seq(0, max(spike_diff_summary$time), by = 2))+#break by two 
theme_minimal() +theme(legend.title = element_blank())
```
Now we want to find for each mouse and across sessions, what's the time interval that the average spike rate have the greatest difference among feedback type 1 and feedback type -1.
```{r}
window_size <- 10

### 1. Find the Best 10-Time-Spot Interval for Each Mouse ###

# Function to find the best 10-time-spot interval per mouse
find_max_spike_diff_interval <- function(df) {
  df <- df %>% arrange(time)  # Ensure data is sorted by time
  
  max_diff <- -Inf
  best_start <- NA
  best_end <- NA
  
  for (i in 1:(nrow(df) - window_size + 1)) {
    #the difference sum
    interval_sum <- sum(df$spike_diff[i:(i + window_size - 1)])
    #if we find the max difference, then we let the best start as i, best end as i+9
    if (interval_sum > max_diff) {
      max_diff <- interval_sum
      best_start <- df$time[i]
      best_end <- df$time[i + window_size - 1]
    }
  }
  #record this best interval back to the dataframe
  return(tibble(name = df$name[1], start_time = best_start, end_time = best_end, max_diff = max_diff))
}

# Compute spike difference for each mice
mouse_spike_diff <- mouse_avg_spike_data %>%
  spread(feedback_type, avg_spike) %>%
  mutate(spike_diff = `Feedback 1` - `Feedback -1`)  

# Apply function to each mouse,group by name
max_diff_intervals_mice <- mouse_spike_diff %>%
  group_by(name) %>%
  group_split() %>%
  map_dfr(find_max_spike_diff_interval)

### 2. Find the Best 10-Time-Spot Interval Across All Sessions ###

# Compute the difference between feedback types for all sessions
spike_diff_summary <- final_spike_summary %>%
  spread(feedback_type, final_avg_spike) %>%
  mutate(spike_diff = `Feedback 1` - `Feedback -1`)

#Still we use that function to find the max interval for average spike differences
max_spike_diff_interval_sessions <- find_max_spike_diff_interval(spike_diff_summary)

### 3. Print the Results ###

print("Max Spike Difference Intervals for Each Mouse:")
print(max_diff_intervals_mice)

print("Max Spike Difference Interval Across All Sessions:")
print(max_spike_diff_interval_sessions)

```


Clearly is around 24-33. Here are 2 choice we can investigate on: first is to generate the average spike rate during 20-30, second one is investigate on seperate mice.
**Cori:18 to 27 (sessions 1-3), Forssmann:22 to 31 (sessions 4-7), Hench:25 to 34 (sessions 8-11), Lederberg:24 to 33 (sessions 12-18)**

See the comparison of average spike rate of session 18 and session 1.
```{r}
#similarly, here we just changed the previous code into only emphasize on session 18 and 1
average_spike_list_18 <- bind_rows(lapply(18:18, function(s) {
  data <- session[[s]] 
  feedback_1_indices <- which(data$feedback_type == 1)
  feedback_neg1_indices <- which(data$feedback_type == -1)
  feedback_1_trials <- data$spks[feedback_1_indices] 
  feedback_neg1_trials <- data$spks[feedback_neg1_indices]
  avg_spike_1 <- colMeans(Reduce("+", feedback_1_trials) / length(feedback_1_trials))
  avg_spike_neg1 <- colMeans(Reduce("+", feedback_neg1_trials) / length(feedback_neg1_trials))
  tibble(
    time = rep(1:40, 2),  # 40 time spot
    avg_spike = c(avg_spike_1, avg_spike_neg1),
    feedback_type = rep(c("Feedback 1", "Feedback -1"), each = 40),name=rep(session[[s]]$mouse_name,80)
  )
}))
final_spike_summary_18 <- average_spike_list_18 %>%
  group_by(time, feedback_type) %>%
  summarise(
    final_avg_spike = mean(avg_spike, na.rm = TRUE),
    .groups = "drop"
  )

ggplot(final_spike_summary_18, aes(x = time, y = final_avg_spike, color = feedback_type)) +
  geom_line(size = 1) +
  labs(title = "Comparison of Average Spike Rate (Feedback 1 vs Feedback -1) for session 18 across time",
       x = "Time", y = "Final Average Spike Rate") +
  theme_minimal() +
  scale_color_manual(values = c("Feedback 1" = "blue", "Feedback -1" = "red")) +
  theme(legend.title = element_blank())  



#This is for session 1
average_spike_list_1 <- bind_rows(lapply(1:1, function(s) {
  data <- session[[s]] 
  feedback_1_indices <- which(data$feedback_type == 1)
  feedback_neg1_indices <- which(data$feedback_type == -1)
  feedback_1_trials <- data$spks[feedback_1_indices] 
  feedback_neg1_trials <- data$spks[feedback_neg1_indices]
  avg_spike_1 <- colMeans(Reduce("+", feedback_1_trials) / length(feedback_1_trials))
  avg_spike_neg1 <- colMeans(Reduce("+", feedback_neg1_trials) / length(feedback_neg1_trials))
  tibble(
    time = rep(1:40, 2),  # 40 time spot
    avg_spike = c(avg_spike_1, avg_spike_neg1),
    feedback_type = rep(c("Feedback 1", "Feedback -1"), each = 40),name=rep(session[[s]]$mouse_name,80)
  )
}))
final_spike_summary_1 <- average_spike_list_1 %>%
  group_by(time, feedback_type) %>%
  summarise(
    final_avg_spike = mean(avg_spike, na.rm = TRUE),
    .groups = "drop"
  )

# print  the final result
ggplot(final_spike_summary_1, aes(x = time, y = final_avg_spike, color = feedback_type)) +
  geom_line(size = 1) +
  labs(title = "Comparison of Average Spike Rate (Feedback 1 vs Feedback -1) for session 1 across time",
       x = "Time", y = "Final Average Spike Rate") +
  theme_minimal() +
  scale_color_manual(values = c("Feedback 1" = "blue", "Feedback -1" = "red")) +
  theme(legend.title = element_blank())  


```




**This two graphs infers that, this type of predictor might be efficient for session 1, since the trend of session 1 is really similar to the whole trend of mice Cori. However, it may lose some predictive ability for session 18 since the general trend is not that similar.**
We have been seen that the average spike rate between each session is really different, so how to integrate the data so that it may enhance the predicitivity? My idea is to turn the average spike rate into (average spike rate of that trail/average spike rate across session), so we can know how is the performance of the average spike rate of that trail interm of the whole session. Since we already see that the average spike rate for feedback type 1 is greater than feedback type negative 1, then the good performance average spike rate(the high spike rate in the session) should be considered as success feedback type.

# Section 3 Data integration

**Base data**
Since we already investigated that different groups of contrast left and right will lead to different success rate and average spike rate for feedback 1 and feedback negative 1 become really different in some time intervals for different mouse(also different in whole time).We can build a basis dataframe which involves the feedback type, the decision(defined as 1 when contrast left bigger than contrast right...Showed in code below) and the average spike rate.
```{r}
all_dat <- bind_rows(lapply(1:18, function(s) {
  data <- session[[s]]
  n_obs <- length(data$feedback_type)
  
  tibble(
    feedback_type = as.factor(data$feedback_type),decision = case_when(
      data$contrast_left > data$contrast_right ~ '1',
      data$contrast_left < data$contrast_right ~ '2',
      data$contrast_left == 0 & data$contrast_right == 0 ~ '3',
      TRUE ~ '4'
    ) %>% as.factor(),
    
    avg_spikes = sapply(1:n_obs, function(i) mean(rowSums(data$spks[[i]]), na.rm = TRUE))
  )
}))
n=5081
print(all_dat)
```
Also we can build a dataframe which only cares about the average spike rate between time spot from 24 to 33.
**Improved data**
```{r}
all_dat_alternative <- bind_rows(lapply(1:18, function(s) {
  data <- session[[s]]
  n_obs <- length(data$feedback_type)
  avg_spike_trials <- sapply(1:length(data$spks), function(trial) {
    trial_spikes <- data$spks[[trial]] 
    # get the time column from 24 to 33, and get the average
    mean_spike <- mean(trial_spikes[, 24:33], na.rm = TRUE)  
    return(mean_spike)
  })
  tibble(
    feedback_type = as.factor(data$feedback_type),decision = case_when(
      data$contrast_left > data$contrast_right ~ '1',
      data$contrast_left < data$contrast_right ~ '2',
      data$contrast_left == 0 & data$contrast_right == 0 ~ '3',
      TRUE ~ '4'
    ) %>% as.factor(),
    
    avg_spikes =avg_spike_trials )
  }))
print(all_dat_alternative)



```


**Dataframe for Lederberg**, which emphasize on time spot between 15 to 35, and the average spike changed into **Norm_spike**=(average spike rate of the trail divided by the average spike rate of that session)
```{r}
Norm_spike_24_33_Lederberg <- lapply(12:18, function(s) {
  data <- session[[s]]  # get data from each session (12-18)
  # write the function which calculate the session average from time spot 24 to 33.
  session_avg_spike <- mean(sapply(data$spks, function(trial_spikes) {
    mean(trial_spikes[, 24:33])
  }))
  # calculate the average spikes trail by trail from time spot 24-33.
  avg_spike_trials <- sapply(1:length(data$spks), function(trial) {
    trial_spikes <- data$spks[[trial]] 
    mean_spike <- mean(trial_spikes[, 24:33], na.rm = TRUE)
  # Calculate the normalized spikes for each trial
    norm_mean_spike <-  mean_spike / session_avg_spike
    return(norm_mean_spike)
  })
  # build dataframe，which include session id, feedback type and the Norm_spike 
  tibble(
    session_id = s,decision = case_when(
      data$contrast_left > data$contrast_right ~ '1',
      data$contrast_left < data$contrast_right ~ '2',
      data$contrast_left == 0 & data$contrast_right == 0 ~ '3',
      TRUE ~ '4'
    ) %>% as.factor(),
    feedback_type = as.factor(data$feedback_type),  
    Norm_spike = avg_spike_trials
  )
})
final_spike_L <- bind_rows(Norm_spike_24_33_Lederberg)
print(final_spike_L)
n_L=2032
```

**Dataframe for Cori**Also we can build a dataframe for Cori using the same technics, the difference is just the time spot is from 18 to 27
```{r}
average_spike_18_27_Cori <- lapply(1:3, function(s) {
  data <- session[[s]]  
  # write the function which calculate the session average from time spot 18 to 27.
  session_avg_spike <- mean(sapply(data$spks, function(trial_spikes) {
    mean(trial_spikes[, 18:27], na.rm = TRUE)
  }), na.rm = TRUE)

  #calculate the average spikes trail by trail from time spot 18-27.
  avg_spike_trials <- sapply(1:length(data$spks), function(trial) {
    trial_spikes <- data$spks[[trial]] 
    mean_spike <- mean(trial_spikes[, 18:27], na.rm = TRUE)
    
    #calculate the normalize one
    norm_mean_spike <-  mean_spike / session_avg_spike
    return(norm_mean_spike)
  })
  # build dataframe，which include session id, feedback type and the Norm_spike
  tibble(
    session_id = s,decision = case_when(
      data$contrast_left > data$contrast_right ~ '1',
      data$contrast_left < data$contrast_right ~ '2',
      data$contrast_left == 0 & data$contrast_right == 0 ~ '3',
      TRUE ~ '4'
    ) %>% as.factor(),
    feedback_type = as.factor(data$feedback_type),  
    Norm_spike = avg_spike_trials
  )
})
final_spike_avg_Cori <- bind_rows(average_spike_18_27_Cori)
print(final_spike_avg_Cori)
n_Cori=593





```
**integrate the test data given by the instructor** using same technic
```{r}
test_data_project_session1<- bind_rows(lapply(1, function(s) {
  data <- sta141A_test_data[[s]]  
  session_avg_spike <- mean(sapply(data$spks, function(trial_spikes) {
    mean(trial_spikes[, 18:27])
  }))
  # calculate the average spikes trail by trail from time spot 18-27.
  avg_spike_trials <- sapply(1:length(data$spks), function(trial) {
    trial_spikes <- data$spks[[trial]] 
    mean_spike <- mean(trial_spikes[, 18:27], na.rm = TRUE)  
    norm_mean_spike <- mean_spike / session_avg_spike
    return(norm_mean_spike)})
  tibble(
    session_id = s,decision = case_when(
      data$contrast_left > data$contrast_right ~ '1',
      data$contrast_left < data$contrast_right ~ '2',
      data$contrast_left == 0 & data$contrast_right == 0 ~ '3',
      TRUE ~ '4'
    ) %>% as.factor(),
    feedback_type = as.factor(data$feedback_type),
    Norm_spike = avg_spike_trials
  )
}))

#also same thing for session 18
test_data_project_session18<- bind_rows(lapply(2, function(s) {
  data <- sta141A_test_data[[s]]
  session_avg_spike <- mean(sapply(data$spks, function(trial_spikes) {
    mean(trial_spikes[, 24:33])
  }))
  # calculate the average spikes trail by trail from time spot 24-33.
  avg_spike_trials <- sapply(1:length(data$spks), function(trial) {
    trial_spikes <- data$spks[[trial]] 
    mean_spike <- mean(trial_spikes[, 24:33], na.rm = TRUE)  
    norm_mean_spike <- mean_spike / session_avg_spike
    return(norm_mean_spike)})
  tibble(
    session_id = s,decision = case_when(
      data$contrast_left > data$contrast_right ~ '1',
      data$contrast_left < data$contrast_right ~ '2',
      data$contrast_left == 0 & data$contrast_right == 0 ~ '3',
      TRUE ~ '4'
    ) %>% as.factor(),
    feedback_type = as.factor(data$feedback_type),
    Norm_spike = avg_spike_trials
  )
}))
print("Dataframe for Session 1 100 trials")
print(test_data_project_session1)
print("Dataframe for Session 18 100 trials")
print(test_data_project_session18)
```








# Section 4 Prediction



Use the dataframes build previously, we can build 3 models based on average spikes throughout all time, specific time interval for Lederberg and Specific time interval for Cori.
*Here we use logistic regression model*

**Build a base model which depends on average spike rate throughout time and the decisions.**
```{r}
set.seed(101)
sample <- sample.int(n = n, size = floor(.8 * n), replace = F)
train1 <- all_dat[sample, ]
test1  <- all_dat[-sample, ]
fit1 <- glm(feedback_type~avg_spikes+decision, data = train1, family="binomial")
summary(fit1)
pred1 <- predict(fit1, test1 %>% select(-feedback_type), type = 'response')
prediction1 <- ifelse(pred1 > 0.5, 1, -1)
firstmodelerror=mean(prediction1 != test1$feedback_type)
print(paste("The accuracy for base model on all sessions is:", round((1-firstmodelerror) * 100, 2), "%"))
```
**Build a base(improvement) model which depends on average spike rate through time interval from 20 to 30 and the decisions.**
```{r}
set.seed(101)
sample <- sample.int(n = n, size = floor(.8 * n), replace = F)
train1_alternative <- all_dat_alternative[sample, ]
test1_alternative  <- all_dat_alternative[-sample, ]
fit1_alternative <- glm(feedback_type~avg_spikes+decision, data = train1_alternative, family="binomial")
summary(fit1_alternative)
pred1_alternative <- predict(fit1_alternative, test1_alternative %>% select(-feedback_type), type = 'response')
prediction1_alternative <- ifelse(pred1_alternative > 0.5, 1, -1)
firstmodelerror_alternative=mean(prediction1_alternative != test1_alternative$feedback_type)
print(paste("The accuracy for the improved base model on all sessions is:", round((1-firstmodelerror_alternative) * 100, 2), "%"))



```
**Some insights:This model is a little bit better than the previous one. However, since the average spike rate for every mice is different. So one thing will probably happened: Even though one spike rate may be significant higher than the spike rate in the same session, it may still can't consider as a high spike rate among all sessions. So in the end this average spike rate may considered as an indicator of feedback negative one, namely failure. So one way, if we want to build the prediction model applied for whole dataset, we may need to change the scale of the spike rate. For example, we investigate on  average spike rate ratio instead of average spike, in which the average spike ratio is calculated by (the average spike rate for this trail/the average spike rate for this session), so every indicator is normalize and it only shows how high it is in its session.**

*However, we can first build prediction model for single mice*

**Model for Lederberg**
```{r}
set.seed(101)
sample <- sample.int(n = n_L, size = floor(.8 * n_L), replace = F)
train0 <- final_spike_L[sample, ]
train0 <- train0 %>% select(-session_id)
test0 <- final_spike_L[-sample, ]
test0 <- test0 %>% select(-session_id)
fit0 <- glm(feedback_type~Norm_spike+decision, data = train0, family="binomial")
summary(fit0)
pred0 <- predict(fit0, test0 %>% select(-feedback_type), type = 'response')
prediction0 <- ifelse(pred0 > 0.5, 1, -1)
Lederbergmodelerror=mean(prediction0 != test0$feedback_type)
print(paste("The accuracy for Lederberg model on its sessions is:", round((1-Lederbergmodelerror) * 100, 2), "%"))
```



**Model for Cori**
```{r}
set.seed(101)
sample <- sample.int(n = n_Cori, size = floor(.8 * n_Cori), replace = F)
trainCori <- final_spike_avg_Cori[sample, ]
trainCori <- trainCori %>% select(-session_id)
testCori <- final_spike_avg_Cori[-sample, ]
testCori <- testCori %>% select(-session_id)
fitCori <- glm(feedback_type~Norm_spike+decision, data = trainCori, family="binomial")
summary(fitCori)
predCori <- predict(fitCori, testCori %>% select(-feedback_type), type = 'response')
predictionCori <- ifelse(predCori > 0.5, 1, -1)
Corimodelerror=mean(predictionCori != testCori$feedback_type)
print(paste("The accuracy for cori model on its sessions is:", round((1-Corimodelerror) * 100, 2), "%"))

```


# Section 5 Prediction performance
```{r}
# Function to compute and plot confusion matrix
plot_confusion_matrix <- function(pred, actual, title) {
  cm <- confusionMatrix(factor(pred, levels = c(-1,1)), factor(actual, levels = c(-1,1)))
  plt <- as.data.frame(cm$table)
  
  ggplot(plt, aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile() + 
    geom_text(aes(label = Freq), color = "black", size = 5) +
    scale_fill_gradient(low = "white", high = "#009194") +
    labs(title = title, x = "Actual", y = "Predicted") +
    theme_minimal()
}
# Print confusion tables
print(table(Predicted = prediction1, Actual = test1$feedback_type))
print(table(Predicted = prediction1_alternative, Actual = test1_alternative$feedback_type))
print(table(Predicted = prediction0, Actual = test0$feedback_type))
print(table(Predicted = predictionCori, Actual = testCori$feedback_type))
# Plot confusion matrices
plot_confusion_matrix(prediction1, test1$feedback_type, "Confusion Matrix - Base model")
plot_confusion_matrix(prediction1_alternative, test1_alternative$feedback_type, "Confusion Matrix - Base model within 20-30 time interval")
plot_confusion_matrix(prediction0, test0$feedback_type, "Confusion Matrix - Model for Lederberg")
plot_confusion_matrix(predictionCori, testCori$feedback_type, "Confusion Matrix - Model for Cori")


```




```{r}
# Base model: Compute ROC and AUC
pr1 <- prediction(pred1, test1$feedback_type)  # Use probabilities, not binary labels
prf1 <- performance(pr1, measure = "tpr", x.measure = "fpr")
auc1 <- performance(pr1, measure = "auc")
auc1 <- auc1@y.values[[1]]

pr1_alternative <- prediction(pred1_alternative, test1_alternative$feedback_type)  # Use probabilities, not binary labels
prf1_alternative <- performance(pr1_alternative, measure = "tpr", x.measure = "fpr")
auc1_alternative <- performance(pr1_alternative, measure = "auc")
auc1_alternative <- auc1_alternative@y.values[[1]]

# Model Lederberg: Compute ROC and AUC
pr2 <- prediction(pred0, test0$feedback_type)  # Use probabilities
prf2 <- performance(pr2, measure = "tpr", x.measure = "fpr")
auc2 <- performance(pr2, measure = "auc")
auc2 <- auc2@y.values[[1]]

#model for Cori
prcori <- prediction(predCori, testCori$feedback_type)  # Use probabilities
prfcori <- performance(prcori, measure = "tpr", x.measure = "fpr")
auccori <- performance(prcori, measure = "auc")
auccori <- auccori@y.values[[1]]


# Bias Guess: Always predict class '1'
predBias <- rep(1, length(test1$feedback_type))  # Always predicting class 1
pr0 <- prediction(predBias, test1$feedback_type)
prf0 <- performance(pr0, measure = "tpr", x.measure = "fpr")
auc0 <- performance(pr0, measure = "auc")
auc0 <- auc0@y.values[[1]]

# Print AUC Values
print(paste("AUC for Base model:", auc1))
print(paste("AUC for Base model(within time interval 20 to 30):", auc1_alternative))
print(paste("AUC for Model Lederberg:", auc2))
print(paste("AUC for Model Cori:", auccori))

# Plot ROC Curves
plot(prf1,  col = 'blue',main = 'ROC Curve between 4 models')
plot(prf1_alternative, add= TRUE, col = 'black')
plot(prf2, add= TRUE, col = 'red')
plot(prfcori, add = TRUE, col = 'purple')
plot(prf0, add = TRUE, col = 'green')

legend("bottomright", legend = c("Base model","Base model in time interval 20-30", "Model for Lederberg","model for Cori", "Bias Guess"), 
       col = c("blue","black", "red","purple" ,"green"), lty = 1, cex = 0.8)
```
We see that the Area over curve for my models are much higer than the base model which based on simple predictor(the average spike rate).


**Test the Lederberg model’s performance on 50 random trails from session 18**
```{r}
set.seed(101)  # Ensure reproducibility

# Filter trials from session 18
session_18_data <- final_spike_L %>% filter(session_id == 18)

# Randomly select 50 trials
sample_trials <- session_18_data %>% sample_n(50)

# Prepare test data (remove session_id and feedback_type for prediction)
test_features <- sample_trials %>% select(-session_id, -feedback_type)

# Make predictions using the trained model
pred_probs <- predict(fit0, test_features, type = 'response')

# Convert probabilities to class predictions (Threshold = 0.5)
predictions <- ifelse(pred_probs > 0.5, 1, -1)

# Compare with actual values
actuals <- sample_trials$feedback_type
test_error <- mean(predictions != actuals)
accuracyLederberg=1-test_error
```

prediction result(accuracy, confusion matrix and AUROC）
```{r}
# Print model error rate on session 18 random trials
print(paste("the accuracy for my Lederberg model is :", round(accuracyLederberg * 100, 2), "%"))


conf_matrix <- confusionMatrix(factor(predictions, levels = c(-1, 1)), 
                               factor(actuals, levels = c(-1, 1)))
conf_matrix$table


roc_obj <- roc(actuals, pred_probs, levels = c(-1, 1), direction = "<")

ggplot(data = data.frame(fpr = roc_obj$specificities, tpr = roc_obj$sensitivities), 
       aes(x = 1 - fpr, y = tpr)) +
  geom_line(color = "blue", size = 1.2) +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(title = "ROC Curve for Lederberg model", x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal()

cat("AUC for Lederberg's Model on Session 18 is:", auc(roc_obj), "\n")

```
**Test the Cori model’s performance on 50 random trails from session 1**
```{R}
set.seed(101)  # Ensure reproducibility

# Filter trials from session 1
session_1_data <- final_spike_avg_Cori %>% filter(session_id == 1)

# Randomly select 50 trials
sample_trials_1 <- session_1_data %>% sample_n(50)

# Prepare test data (remove session_id and feedback_type for prediction)
test_features_1 <- sample_trials_1 %>% select(-session_id, -feedback_type)

# Make predictions using the trained model
pred_probs_cori <- predict(fitCori, test_features_1, type = 'response')

# Convert probabilities to class predictions (Threshold = 0.5)
predictions_cori <- ifelse(pred_probs_cori > 0.5, 1, -1)

# Compare with actual values
actuals_cori <- sample_trials_1$feedback_type
test_error_cori <- mean(predictions_cori != actuals_cori)
accuracyCori=1-test_error_cori
```
prediction result(accuracy, confusion matrix and AUROC）
```{r}
# Print accuracy result
print(paste("The accuracy for Cori's model on session 1 is:", round(accuracyCori * 100, 2), "%"))

# Compute confusion matrix
conf_matrix_cori <- confusionMatrix(factor(predictions_cori, levels = c(-1, 1)), 
                                    factor(actuals_cori, levels = c(-1, 1)))
print(conf_matrix_cori$table)

# Compute ROC Curve and AUC
roc_obj_cori <- roc(actuals_cori, pred_probs_cori, levels = c(-1, 1), direction = "<")

# Plot ROC Curve for Cori's model
ggplot(data = data.frame(fpr = roc_obj_cori$specificities, tpr = roc_obj_cori$sensitivities), 
       aes(x = 1 - fpr, y = tpr)) +
  geom_line(color = "blue", size = 1.2) +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(title = "ROC Curve - Cori's Model (Session 1)", x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal()

# Print AUC value
cat("AUC for Cori's Model on Session 1:", auc(roc_obj_cori), "\n")


```

**Test data given by the instructor**
**First we test the 100 trials on session 1**
we defined the whole dataset from session 1 to session 3 previously given as the training set, and defined the new given 100 trails on session 1 as the test set.
```{r}
set.seed(101)
trainCori_final <- final_spike_avg_Cori%>% select(-session_id)
testCori_final <- test_data_project_session1%>% select(-session_id)
fitCori_final <- glm(feedback_type~Norm_spike+decision, data = trainCori_final, family="binomial")
summary(fitCori_final)
predCori_final <- predict(fitCori_final, testCori_final %>% select(-feedback_type), type = 'response')
predictionCori_final <- ifelse(predCori_final > 0.5, 1, -1)
Corimodelerror_final=mean(predictionCori_final != testCori_final$feedback_type)
```
prediction result(accuracy, confusion matrix and AUROC）
```{r}
print(paste("The accuracy for Cori's model on 100 trails from  session 1 is:", round((1-Corimodelerror_final) * 100, 2), "%"))
# Compute confusion matrix
conf_matrix_cori_final <- confusionMatrix(factor(predictionCori_final, levels = c(-1, 1)), 
                                    factor(testCori_final$feedback_type, levels = c(-1, 1)))
print(conf_matrix_cori_final$table)

# Compute ROC Curve and AUC
roc_obj_cori_final <- roc(testCori_final$feedback_type, predCori_final, levels = c(-1, 1), direction = "<")

# Plot ROC Curve for Cori's model
ggplot(data = data.frame(fpr = roc_obj_cori_final$specificities, tpr = roc_obj_cori_final$sensitivities), 
       aes(x = 1 - fpr, y = tpr)) +
  geom_line(color = "blue", size = 1.2) +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(title = "ROC Curve - Cori's Model (100 trials from Session 1 given by the professor)", x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal()

# Print AUC value
cat("AUC for Cori's Model on 100 trials in Session 1 is :", auc(roc_obj_cori_final), "\n")
```
**Then we test on 100 trails from session 18, using the Lederberg model**
```{r}
set.seed(101)
trainLederberg_final <- final_spike_L%>% select(-session_id)
testLederberg_final <- test_data_project_session18%>% select(-session_id)
fitLederberg_final <- glm(feedback_type~Norm_spike+decision, data = trainLederberg_final, family="binomial")
summary(fitLederberg_final)
predLederberg_final <- predict(fitLederberg_final, testLederberg_final %>% select(-feedback_type), type = 'response')
predictionLederberg_final <- ifelse(predLederberg_final > 0.5, 1, -1)
Lederbergmodelerror_final=mean(predictionLederberg_final != testLederberg_final$feedback_type)
```
prediction result(accuracy, confusion matrix and AUROC）
```{r}
print(paste("The accuracy for Lederberg's model on 100 trials from  session 18 is:", round((1-Lederbergmodelerror_final) * 100, 2), "%"))
# Compute confusion matrix
conf_matrix_Lederberg_final <- confusionMatrix(factor(predictionLederberg_final, levels = c(-1, 1)), 
                                    factor(testLederberg_final$feedback_type, levels = c(-1, 1)))
print(conf_matrix_Lederberg_final$table)

# Compute ROC Curve and AUC
roc_obj_Lederberg_final <- roc(testLederberg_final$feedback_type, predLederberg_final, levels = c(-1, 1), direction = "<")

# Plot ROC Curve for Lederberg's model
ggplot(data = data.frame(fpr = roc_obj_Lederberg_final$specificities, tpr = roc_obj_Lederberg_final$sensitivities), 
       aes(x = 1 - fpr, y = tpr)) +
  geom_line(color = "blue", size = 1.2) +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(title = "ROC Curve - Lederberg's Model (100 trials from Session 18 given by the professor)", x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal()

# Print AUC value
cat("AUC for Lederberg's Model on 100 trials in Session 18 is :", auc(roc_obj_Lederberg_final), "\n")


```
Even though the accuracy may looks low, i conclude that this is reasonable since Lederberg spike rate is really different for each session. If we want to improve this model, we may need to only focus on the sessions from 12 to 18 that have the similar trend with session 18.

# Section 6 Discussion 

**Conclusion:**
In this project, we goes through the 18 sessions and want to find is there any way to predict the feedback type by neural activities along with stimuli. By doing data exploration, i successful build a model to predict the feedback type result. 

Test the Lederberg model’s performance on 50 random trails from session 18
Accuracy of 72 % and AOC of 0.7913753

Test the Cori model’s performance on 50 random trails from session 1
Accuracy of 78 % and AOC of 0.8415435

Test data given by professor:
For 100 trials from session 1: We have an accuracy of 75 percent and AOC of 0.7859
For 100 trials form session 18:We have an accuracy of 70 percent and AOC of 0.6793

**Key Findings**
The most significant finding in this project is that when feedback type is 1(namely when the mice success), the average spike rate is similar as when the feedback type is negative 1 before about 10 seconds. However, after 10 seconds, the average spike rate of feedback type 1 become larger than the average spike rate of feedback type negative 1, and the difference become largest between time 24 to time 33.
For each mouse, this pattern behaves differently and even different across the sessions for the same mouse,however, we still can find the time interval where when is the difference become the largest.
Also, we noticed that the mice becomes tried after trails, which showed by the decreasing of average spike rate when trails are increasing for almost all sessions.

The reason why we have an accuracy of about 75 percent for the 100 trails on session1 but only 70 percent for the 100 trials on session 18 is not hard to explained. By seeing in the graph of the average spike rate over time for mouse Cori and mouse Lederberg, we can notice that actually the graph of the average spike rate of feedback 1 and feedback -1 over time in session 1 is really similar to the overall graph which across session 1 to session 3. However, the graph of session 18 didn't really similar to the overall graph which include from session 12 to session 18.

**Talks about limitations:**
Just like what i talk about in the key findings, since every session average spike rate perform differently,even normalize all average spike rate can't put all trails in the same scale. In addition, in some sessions the average spike rate between feedback 1 and feedback -1 may not large differences between the time interval concludes from the graphs that record the average across mice. Also, some outliers may distort the data, give wrong direction to the model. For example, the mouse may stimulated by the environment in some trials which give them a really high spike rate even though the feedback type is -1(failure).

**Potential improvement:**
One improvement is to lay out the graphs which includes the average spike rate for feedback 1 and feedback -1 for all sessions. And for same mouse, for example, session 1 to 3, we want to the overlap of the time interval which have a relatively large difference between the two average spike rate. So by doing so, the integrated dataframe which only focus on this renewed time interval can applied well for all sessions. 
Another improvement we can make is that we may can classify some potential outliers that will become noise for the whole model.
In addition, i didn't investigate what certain type of brain area will contribute to my prediction. However, there should be certain types of brain areas that essential for decision making. We can try to do it by summarize the potential significant brain areas for each mouse, and build a prediction model for each mouse based on these brain areas. 


# Acknowledgement:

**Conversation with chatgpt**: https://chatgpt.com/share/67d88015-e544-8001-938e-16a510f5f186
In addition, within the Exploratory data analysis session, there are some codes which were written involved the help Chatgpt, however, those links of the chats couldn't be copied since there were pictures uploaded within the chats.







































```